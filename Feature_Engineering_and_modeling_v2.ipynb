{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fae442b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# importing important libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Functions import Basic_info_func, Remove_outliers_with_lof, Select_k_best_features, Apply_pca\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.feature_selection import mutual_info_regression, SelectKBest\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f6ad9",
   "metadata": {},
   "source": [
    "Path = /OneDrive/Desktop/MS-AAi/Course_500_Probability/Project_AAI500-A1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b89890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset \n",
    "df = pd.read_csv('./Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f63a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_variables = df.drop('critical_temp', axis = 1)\n",
    "target_variable = df['critical_temp']\n",
    "    \n",
    "train_X, test_X, train_y, test_y = train_test_split(independent_variables, target_variable, \n",
    "                                                    test_size=0.2, random_state=0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690880d1",
   "metadata": {},
   "source": [
    "### Outlier Detection and removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "008803fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Determine the number of features and calculate the number of subplots needed\n",
    "# num_features = len(train_X)\n",
    "\n",
    "# # Create the subplots\n",
    "# sns.set_style('darkgrid')\n",
    "# fig, ax = plt.subplots(9, 9, figsize=(15, 10))\n",
    "\n",
    "# # Flatten the axes array for easy iteration\n",
    "# ax_flat = ax.flatten()\n",
    "\n",
    "# # Iterate over each element property and corresponding axis\n",
    "# for property_name, axis in zip(train_X, ax_flat):\n",
    "#     sns.kdeplot(data=df, x=property_name , ax=axis)\n",
    "\n",
    "# # Hide empty subplots if any\n",
    "# for axis in ax_flat[num_features:]:\n",
    "#     axis.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.suptitle('Distribution of independent features', fontsize=16, y=1.05)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a278f0",
   "metadata": {},
   "source": [
    "Notice that we have features that seem to have some extereme values, such as wtd_range_FusionHeat and mean_Density. In order to tackle these extremet points we can use a Machine lerning approch named local outlier factor that can help us predicting outliers and removing them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28bb97ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before outlier removal:\n",
      "(17010, 82)\n",
      "\n",
      "Shape after outlier removal:\n",
      "(16159, 82)\n"
     ]
    }
   ],
   "source": [
    "new_train_X, new_train_y  = Remove_outliers_with_lof(train_X, train_y, contamination = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e29a48b",
   "metadata": {},
   "source": [
    "#### Features Selection\n",
    "\n",
    "In the data analysis part we observed that our entire data has many highly colinear features that causes multi colinearity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73cd0cb6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores of top 30 features in descending order:\n",
      "std_fie                        0.930362\n",
      "gmean_Density                  0.924645\n",
      "entropy_atomic_mass            0.911777\n",
      "std_ThermalConductivity        0.910415\n",
      "range_ElectronAffinity         0.905139\n",
      "range_fie                      0.896992\n",
      "entropy_atomic_radius          0.889684\n",
      "wtd_gmean_Valence              0.873904\n",
      "entropy_Density                0.872597\n",
      "wtd_mean_Valence               0.870127\n",
      "range_Density                  0.869347\n",
      "std_ElectronAffinity           0.860537\n",
      "entropy_FusionHeat             0.860131\n",
      "entropy_ElectronAffinity       0.858371\n",
      "mean_ThermalConductivity       0.851045\n",
      "gmean_FusionHeat               0.850557\n",
      "wtd_gmean_Density              0.828712\n",
      "std_atomic_radius              0.824413\n",
      "gmean_ThermalConductivity      0.823578\n",
      "gmean_ElectronAffinity         0.821363\n",
      "entropy_Valence                0.820703\n",
      "range_atomic_mass              0.812493\n",
      "range_atomic_radius            0.810315\n",
      "mean_FusionHeat                0.807334\n",
      "entropy_fie                    0.804873\n",
      "wtd_gmean_FusionHeat           0.798382\n",
      "range_FusionHeat               0.796687\n",
      "wtd_std_ThermalConductivity    0.796632\n",
      "gmean_atomic_mass              0.786898\n",
      "mean_ElectronAffinity          0.783565\n",
      "Name: Scores, dtype: float64\n",
      "Selected top 30 features:\n"
     ]
    }
   ],
   "source": [
    "# Assuming train_X and train_y are already defined and contain the training data\n",
    "\n",
    "k = 30  # Number of features to select\n",
    "X_new, scores = Select_k_best_features(new_train_X, new_train_y, k=k, score_func=mutual_info_regression)\n",
    "\n",
    "print(f\"Selected top {k} features:\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eddeffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = X_new.columns\n",
    "test_X = test_X[feature_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d6546ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_train_X = scaler.fit_transform(X_new)\n",
    "scaled_test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fabd7d5",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04ef1b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results \n",
      " - - - - - - - - - - - - - - - - - - - - \n",
      "Training RMSE: 19.79507\n",
      "Training MAE: 15.08588\n",
      "Training R2_score: 0.66478\n",
      "Testing results \n",
      " - - - - - - - - - - - - - - - - - - - - \n",
      "Testing RMSE: 20.13932\n",
      "Testing MAE: 15.46959\n",
      "Testing R2_score: 0.65152\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Initialize the linear regression model\n",
    "simple_linear_regression = LinearRegression()\n",
    "\n",
    "# Step 3: Fit the model on the scaled training data\n",
    "simple_linear_regression.fit(scaled_train_X, new_train_y)\n",
    "\n",
    "# Step 4: Predict on the training set\n",
    "train_preds = simple_linear_regression.predict(scaled_train_X)\n",
    "\n",
    "# Training evaluation\n",
    "print('Training results', '\\n', '- '*20)\n",
    "RMSE = np.sqrt(mean_squared_error(new_train_y, train_preds))\n",
    "MAE = mean_absolute_error(new_train_y, train_preds)\n",
    "R2_score = r2_score(new_train_y, train_preds)\n",
    "\n",
    "print(f'Training RMSE: {RMSE:.5f}')\n",
    "print(f'Training MAE: {MAE:.5f}')\n",
    "print(f'Training R2_score: {R2_score:.5f}')\n",
    "\n",
    "\n",
    "# Testing Results\n",
    "test_preds = simple_linear_regression.predict(scaled_test_X)\n",
    "\n",
    "#Testing evaluation\n",
    "print('Testing results', '\\n', '- '*20)\n",
    "RMSE_test = np.sqrt(mean_squared_error(test_y, test_preds))\n",
    "MAE_test = mean_absolute_error(test_y, test_preds)\n",
    "R2_score_test = r2_score(test_y, test_preds)\n",
    "\n",
    "print(f'Testing RMSE: {RMSE_test:.5f}')\n",
    "print(f'Testing MAE: {MAE_test:.5f}')\n",
    "print(f'Testing R2_score: {R2_score_test:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba88024",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11647ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results \n",
      " - - - - - - - - - - - - - - - - - - - - \n",
      "Training RMSE: 9.61332\n",
      "Training MAE: 6.44550\n",
      "Training R2_score: 0.92094\n",
      "Testing results \n",
      " - - - - - - - - - - - - - - - - - - - - \n",
      "Testing RMSE: 11.60691\n",
      "Testing MAE: 7.54370\n",
      "Testing R2_score: 0.88425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Step 2: Initialize the Gradient Boosting Regressor model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "\n",
    "# Step 3: Fit the model on the scaled training data\n",
    "gb_model.fit(scaled_train_X, new_train_y)\n",
    "\n",
    "# Step 4: Predict on the training set\n",
    "train_preds = gb_model.predict(scaled_train_X)\n",
    "\n",
    "# Training evaluation\n",
    "print('Training results', '\\n', '- '*20)\n",
    "RMSE = np.sqrt(mean_squared_error(new_train_y, train_preds))\n",
    "MAE = mean_absolute_error(new_train_y, train_preds)\n",
    "R2_score = r2_score(new_train_y, train_preds)\n",
    "\n",
    "print(f'Training RMSE: {RMSE:.5f}')\n",
    "print(f'Training MAE: {MAE:.5f}')\n",
    "print(f'Training R2_score: {R2_score:.5f}')\n",
    "\n",
    "# Step 5: Predict on the testing set\n",
    "test_preds = gb_model.predict(scaled_test_X)\n",
    "\n",
    "# Testing evaluation\n",
    "print('Testing results', '\\n', '- '*20)\n",
    "RMSE_test = np.sqrt(mean_squared_error(test_y, test_preds))\n",
    "MAE_test = mean_absolute_error(test_y, test_preds)\n",
    "R2_score_test = r2_score(test_y, test_preds)\n",
    "\n",
    "print(f'Testing RMSE: {RMSE_test:.5f}')\n",
    "print(f'Testing MAE: {MAE_test:.5f}')\n",
    "print(f'Testing R2_score: {R2_score_test:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7536c4",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e894713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results \n",
      " - - - - - - - - - - - - - - - - - - - - \n",
      "Training RMSE: 6.76219\n",
      "Training MAE: 4.27993\n",
      "Training R2_score: 0.96088\n",
      "Testing results \n",
      " - - - - - - - - - - - - - - - - - - - - \n",
      "Testing RMSE: 11.67695\n",
      "Testing MAE: 6.82261\n",
      "Testing R2_score: 0.88285\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Step 2: Initialize the XGBoost Regressor model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=500, learning_rate=0.9, max_depth=3, random_state=42)\n",
    "\n",
    "# Step 3: Fit the model on the scaled training data\n",
    "xgb_model.fit(scaled_train_X, new_train_y)\n",
    "\n",
    "# Step 4: Predict on the training set\n",
    "train_preds = xgb_model.predict(scaled_train_X)\n",
    "\n",
    "# Training evaluation\n",
    "print('Training results', '\\n', '- '*20)\n",
    "RMSE = np.sqrt(mean_squared_error(new_train_y, train_preds))\n",
    "MAE = mean_absolute_error(new_train_y, train_preds)\n",
    "R2_score = r2_score(new_train_y, train_preds)\n",
    "\n",
    "print(f'Training RMSE: {RMSE:.5f}')\n",
    "print(f'Training MAE: {MAE:.5f}')\n",
    "print(f'Training R2_score: {R2_score:.5f}')\n",
    "\n",
    "# Step 5: Predict on the testing set\n",
    "test_preds = xgb_model.predict(scaled_test_X)\n",
    "\n",
    "# Testing evaluation\n",
    "print('Testing results', '\\n', '- '*20)\n",
    "RMSE_test = np.sqrt(mean_squared_error(test_y, test_preds))\n",
    "MAE_test = mean_absolute_error(test_y, test_preds)\n",
    "R2_score_test = r2_score(test_y, test_preds)\n",
    "\n",
    "print(f'Testing RMSE: {RMSE_test:.5f}')\n",
    "print(f'Testing MAE: {MAE_test:.5f}')\n",
    "print(f'Testing R2_score: {R2_score_test:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1a2c2",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d6de56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 9.00926\ttraining's l1: 5.91684\tvalid_1's rmse: 11.15\tvalid_1's l1: 7.10398\n",
      "Training results:\n",
      "Training RMSE: 9.00926\n",
      "Training MAE: 5.91684\n",
      "Training R2_score: 0.93056\n",
      "\n",
      "Testing results:\n",
      "Testing RMSE: 11.15004\n",
      "Testing MAE: 7.10398\n",
      "Testing R2_score: 0.89318\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Prepare the dataset for LightGBM\n",
    "train_data = lgb.Dataset(scaled_train_X, label=new_train_y)\n",
    "test_data = lgb.Dataset(scaled_test_X, label=test_y, reference=train_data)\n",
    "\n",
    "# Define the parameters\n",
    "params = {\n",
    "    'objective': 'regression',  # correct objective for regression\n",
    "    'metric': ['rmse', 'mae'],  # appropriate metrics\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1,\n",
    "    'learning_rate': 0.01,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Train the model with early stopping callback\n",
    "model = lgb.train(\n",
    "    params, \n",
    "    train_data, \n",
    "    num_boost_round=1000, \n",
    "    valid_sets=[train_data, test_data], \n",
    "    callbacks=[early_stopping(stopping_rounds=100)]\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "train_preds = model.predict(scaled_train_X, num_iteration=model.best_iteration)\n",
    "test_preds = model.predict(scaled_test_X, num_iteration=model.best_iteration)\n",
    "\n",
    "# Evaluate the model\n",
    "train_rmse = mean_squared_error(new_train_y, train_preds, squared=False)\n",
    "train_mae = mean_absolute_error(new_train_y, train_preds)\n",
    "train_r2 = r2_score(new_train_y, train_preds)\n",
    "\n",
    "print('Training results:')\n",
    "print(f'Training RMSE: {train_rmse:.5f}')\n",
    "print(f'Training MAE: {train_mae:.5f}')\n",
    "print(f'Training R2_score: {train_r2:.5f}')\n",
    "\n",
    "test_rmse = mean_squared_error(test_y, test_preds, squared=False)\n",
    "test_mae = mean_absolute_error(test_y, test_preds)\n",
    "test_r2 = r2_score(test_y, test_preds)\n",
    "\n",
    "print('\\nTesting results:')\n",
    "print(f'Testing RMSE: {test_rmse:.5f}')\n",
    "print(f'Testing MAE: {test_mae:.5f}')\n",
    "print(f'Testing R2_score: {test_r2:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22ec8812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best parameters: {'subsample': 0.6, 'n_estimators': 500, 'max_depth': 10, 'learning_rate': 0.03, 'colsample_bytree': 0.6}\n",
      "Training results:\n",
      "Training RMSE: 5.14040\n",
      "Training MAE: 2.96495\n",
      "Training R2_score: 0.97739\n",
      "\n",
      "Testing results:\n",
      "Testing RMSE: 10.10572\n",
      "Testing MAE: 5.71245\n",
      "Testing R2_score: 0.91226\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "    'max_depth': [4, 6, 8, 10],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'n_estimators': [100, 200, 500, 1000]\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', seed=42)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid, \n",
    "                                 scoring='neg_mean_squared_error', n_iter=50, \n",
    "                                 cv=5, verbose=1, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(scaled_train_X, new_train_y)\n",
    "\n",
    "# Best parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Train with the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "model = xgb.XGBRegressor(**best_params, objective='reg:squarederror', seed=42)\n",
    "model.fit(scaled_train_X, new_train_y)\n",
    "\n",
    "# Make predictions\n",
    "train_preds = model.predict(scaled_train_X)\n",
    "test_preds = model.predict(scaled_test_X)\n",
    "\n",
    "# Evaluate the model\n",
    "train_rmse = mean_squared_error(new_train_y, train_preds, squared=False)\n",
    "train_mae = mean_absolute_error(new_train_y, train_preds)\n",
    "train_r2 = r2_score(new_train_y, train_preds)\n",
    "\n",
    "print('Training results:')\n",
    "print(f'Training RMSE: {train_rmse:.5f}')\n",
    "print(f'Training MAE: {train_mae:.5f}')\n",
    "print(f'Training R2_score: {train_r2:.5f}')\n",
    "\n",
    "test_rmse = mean_squared_error(test_y, test_preds, squared=False)\n",
    "test_mae = mean_absolute_error(test_y, test_preds)\n",
    "test_r2 = r2_score(test_y, test_preds)\n",
    "\n",
    "print('\\nTesting results:')\n",
    "print(f'Testing RMSE: {test_rmse:.5f}')\n",
    "print(f'Testing MAE: {test_mae:.5f}')\n",
    "print(f'Testing R2_score: {test_r2:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa54202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
