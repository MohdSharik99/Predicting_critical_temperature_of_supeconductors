{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64d07894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fae442b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "from Functions import Basic_info_func, Remove_outliers_with_lof, Select_k_best_features\n",
    "\n",
    "# importing important libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.feature_selection import mutual_info_regression, SelectKBest\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5cd32f",
   "metadata": {},
   "source": [
    "Path = /OneDrive/Desktop/MS-AAi/Course_500_Probability/Project_AAI500-A1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b89890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset \n",
    "df = pd.read_csv('./Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f63a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_variables = df.drop('critical_temp', axis = 1)\n",
    "target_variable = df['critical_temp']\n",
    "    \n",
    "train_X, test_X, train_y, test_y = train_test_split(independent_variables, target_variable, \n",
    "                                                    test_size=0.2, random_state=0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690880d1",
   "metadata": {},
   "source": [
    "### Outlier Detection and removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008803fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of features and calculate the number of subplots needed\n",
    "num_features = len(train_X)\n",
    "\n",
    "# Create the subplots\n",
    "sns.set_style('darkgrid')\n",
    "fig, ax = plt.subplots(9, 9, figsize=(15, 10))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "ax_flat = ax.flatten()\n",
    "\n",
    "# Iterate over each element property and corresponding axis\n",
    "for property_name, axis in zip(train_X, ax_flat):\n",
    "    sns.kdeplot(data=df, x=property_name , ax=axis)\n",
    "\n",
    "# Hide empty subplots if any\n",
    "for axis in ax_flat[num_features:]:\n",
    "    axis.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Distribution of independent features', fontsize=16, y=1.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a278f0",
   "metadata": {},
   "source": [
    "Notice that we have features that seem to have some extereme values, such as wtd_range_FusionHeat and mean_Density. In order to tackle these extremet points we can use a Machine lerning approch named local outlier factor that can help us predicting outliers and removing them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb97ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_X, new_train_y  = Remove_outliers_with_lof(train_X, train_y, contamination = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e29a48b",
   "metadata": {},
   "source": [
    "#### Features Selection\n",
    "\n",
    "In the data analysis part we observed that our entire data has many highly colinear features that causes multi colinearity. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9ca84",
   "metadata": {},
   "source": [
    "### Linear Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ecef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_train_X = scaler.fit_transform(new_train_X)\n",
    "scaled_test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize the linear regression model\n",
    "simple_linear_regression = LinearRegression()\n",
    "\n",
    "# Step 3: Fit the model on the scaled training data\n",
    "simple_linear_regression.fit(scaled_train_X, new_train_y)\n",
    "\n",
    "# Step 4: Predict on the training set\n",
    "train_preds = simple_linear_regression.predict(scaled_train_X)\n",
    "\n",
    "# Training evaluation\n",
    "print('Training results', '\\n', '- '*20)\n",
    "RMSE = np.sqrt(mean_squared_error(new_train_y, train_preds))\n",
    "MAE = mean_absolute_error(new_train_y, train_preds)\n",
    "R2_score = r2_score(new_train_y, train_preds)\n",
    "\n",
    "print(f'Training RMSE: {RMSE:.5f}')\n",
    "print(f'Training MAE: {MAE:.5f}')\n",
    "print(f'Training R2_score: {R2_score:.5f}')\n",
    "\n",
    "\n",
    "# Testing Results\n",
    "test_preds = simple_linear_regression.predict(scaled_test_X)\n",
    "\n",
    "#Testing evaluation\n",
    "print('Testing results', '\\n', '- '*20)\n",
    "RMSE_test = np.sqrt(mean_squared_error(test_y, test_preds))\n",
    "MAE_test = mean_absolute_error(test_y, test_preds)\n",
    "R2_score_test = r2_score(test_y, test_preds)\n",
    "\n",
    "print(f'Testing RMSE: {RMSE_test:.5f}')\n",
    "print(f'Testing MAE: {MAE_test:.5f}')\n",
    "print(f'Testing R2_score: {R2_score_test:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c509814c",
   "metadata": {},
   "source": [
    "#### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4d420a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Step 2: Initialize the Gradient Boosting Regressor model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "\n",
    "# Step 3: Fit the model on the scaled training data\n",
    "gb_model.fit(scaled_train_X, new_train_y)\n",
    "\n",
    "# Step 4: Predict on the training set\n",
    "train_preds = gb_model.predict(scaled_train_X)\n",
    "\n",
    "# Training evaluation\n",
    "print('Training results', '\\n', '- '*20)\n",
    "RMSE = np.sqrt(mean_squared_error(new_train_y, train_preds))\n",
    "MAE = mean_absolute_error(new_train_y, train_preds)\n",
    "R2_score = r2_score(new_train_y, train_preds)\n",
    "\n",
    "print(f'Training RMSE: {RMSE:.5f}')\n",
    "print(f'Training MAE: {MAE:.5f}')\n",
    "print(f'Training R2_score: {R2_score:.5f}')\n",
    "\n",
    "# Step 5: Predict on the testing set\n",
    "test_preds = gb_model.predict(scaled_test_X)\n",
    "\n",
    "# Testing evaluation\n",
    "print('Testing results', '\\n', '- '*20)\n",
    "RMSE_test = np.sqrt(mean_squared_error(test_y, test_preds))\n",
    "MAE_test = mean_absolute_error(test_y, test_preds)\n",
    "R2_score_test = r2_score(test_y, test_preds)\n",
    "\n",
    "print(f'Testing RMSE: {RMSE_test:.5f}')\n",
    "print(f'Testing MAE: {MAE_test:.5f}')\n",
    "print(f'Testing R2_score: {R2_score_test:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7231b",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a495580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Step 2: Initialize the XGBoost Regressor model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=500, learning_rate=0.9, max_depth=3, random_state=42)\n",
    "\n",
    "# Step 3: Fit the model on the scaled training data\n",
    "xgb_model.fit(scaled_train_X, new_train_y)\n",
    "\n",
    "# Step 4: Predict on the training set\n",
    "train_preds = xgb_model.predict(scaled_train_X)\n",
    "\n",
    "# Training evaluation\n",
    "print('Training results', '\\n', '- '*20)\n",
    "RMSE = np.sqrt(mean_squared_error(new_train_y, train_preds))\n",
    "MAE = mean_absolute_error(new_train_y, train_preds)\n",
    "R2_score = r2_score(new_train_y, train_preds)\n",
    "\n",
    "print(f'Training RMSE: {RMSE:.5f}')\n",
    "print(f'Training MAE: {MAE:.5f}')\n",
    "print(f'Training R2_score: {R2_score:.5f}')\n",
    "\n",
    "# Step 5: Predict on the testing set\n",
    "test_preds = xgb_model.predict(scaled_test_X)\n",
    "\n",
    "# Testing evaluation\n",
    "print('Testing results', '\\n', '- '*20)\n",
    "RMSE_test = np.sqrt(mean_squared_error(test_y, test_preds))\n",
    "MAE_test = mean_absolute_error(test_y, test_preds)\n",
    "R2_score_test = r2_score(test_y, test_preds)\n",
    "\n",
    "print(f'Testing RMSE: {RMSE_test:.5f}')\n",
    "print(f'Testing MAE: {MAE_test:.5f}')\n",
    "print(f'Testing R2_score: {R2_score_test:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a15f6b5",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(scaled_train_X.shape[1],)),\n",
    "    Dropout(0.4),  # Example of adding dropout for regularization\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression task\n",
    "])\n",
    "\n",
    "# Compile the model with appropriate optimizer and loss function\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae', 'mse'])\n",
    "\n",
    "# Print the model summary to understand the architecture and number of parameters\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(scaled_train_X, new_train_y, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate on training set\n",
    "train_preds = model.predict(scaled_train_X)\n",
    "train_rmse = np.sqrt(mean_squared_error(new_train_y, train_preds))\n",
    "train_mae = mean_absolute_error(new_train_y, train_preds)\n",
    "train_r2 = r2_score(new_train_y, train_preds)\n",
    "\n",
    "print('Training results:')\n",
    "print(f'Training RMSE: {train_rmse:.5f}')\n",
    "print(f'Training MAE: {train_mae:.5f}')\n",
    "print(f'Training R2_score: {train_r2:.5f}')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_preds = model.predict(scaled_test_X)\n",
    "test_rmse = np.sqrt(mean_squared_error(test_y, test_preds))\n",
    "test_mae = mean_absolute_error(test_y, test_preds)\n",
    "test_r2 = r2_score(test_y, test_preds)\n",
    "\n",
    "print('\\nTesting results:')\n",
    "print(f'Testing RMSE: {test_rmse:.5f}')\n",
    "print(f'Testing MAE: {test_mae:.5f}')\n",
    "print(f'Testing R2_score: {test_r2:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88584c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ba2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the early stopping callback\n",
    "from lightgbm import early_stopping\n",
    "\n",
    "# Train the model with early stopping callback\n",
    "model = lgb.train(\n",
    "    params, \n",
    "    train_data, \n",
    "    num_boost_round=1000, \n",
    "    valid_sets=[train_data, test_data], \n",
    "    callbacks=[early_stopping(stopping_rounds=100)]\n",
    ")\n",
    "\n",
    "# The rest of the code remains the same\n",
    "train_preds = model.predict(scaled_train_X, num_iteration=model.best_iteration)\n",
    "test_preds = model.predict(scaled_test_X, num_iteration=model.best_iteration)\n",
    "\n",
    "# Evaluate the model\n",
    "train_rmse = mean_squared_error(new_train_y, train_preds, squared=False)\n",
    "train_mae = mean_absolute_error(new_train_y, train_preds)\n",
    "train_r2 = r2_score(new_train_y, train_preds)\n",
    "\n",
    "print('Training results:')\n",
    "print(f'Training RMSE: {train_rmse:.5f}')\n",
    "print(f'Training MAE: {train_mae:.5f}')\n",
    "print(f'Training R2_score: {train_r2:.5f}')\n",
    "\n",
    "test_rmse = mean_squared_error(test_y, test_preds, squared=False)\n",
    "test_mae = mean_absolute_error(test_y, test_preds)\n",
    "test_r2 = r2_score(test_y, test_preds)\n",
    "\n",
    "print('\\nTesting results:')\n",
    "print(f'Testing RMSE: {test_rmse:.5f}')\n",
    "print(f'Testing MAE: {test_mae:.5f}')\n",
    "print(f'Testing R2_score: {test_r2:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ef241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "    'max_depth': [4, 6, 8, 10],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'n_estimators': [100, 200, 500, 1000]\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', seed=42)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid, \n",
    "                                 scoring='neg_mean_squared_error', n_iter=50, \n",
    "                                 cv=5, verbose=1, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(scaled_train_X, new_train_y)\n",
    "\n",
    "# Best parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Train with the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "model = xgb.XGBRegressor(**best_params, objective='reg:squarederror', seed=42)\n",
    "model.fit(scaled_train_X, new_train_y)\n",
    "\n",
    "# Make predictions\n",
    "train_preds = model.predict(scaled_train_X)\n",
    "test_preds = model.predict(scaled_test_X)\n",
    "\n",
    "# Evaluate the model\n",
    "train_rmse = mean_squared_error(new_train_y, train_preds, squared=False)\n",
    "train_mae = mean_absolute_error(new_train_y, train_preds)\n",
    "train_r2 = r2_score(new_train_y, train_preds)\n",
    "\n",
    "print('Training results:')\n",
    "print(f'Training RMSE: {train_rmse:.5f}')\n",
    "print(f'Training MAE: {train_mae:.5f}')\n",
    "print(f'Training R2_score: {train_r2:.5f}')\n",
    "\n",
    "test_rmse = mean_squared_error(test_y, test_preds, squared=False)\n",
    "test_mae = mean_absolute_error(test_y, test_preds)\n",
    "test_r2 = r2_score(test_y, test_preds)\n",
    "\n",
    "print('\\nTesting results:')\n",
    "print(f'Testing RMSE: {test_rmse:.5f}')\n",
    "print(f'Testing MAE: {test_mae:.5f}')\n",
    "print(f'Testing R2_score: {test_r2:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6883455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea443da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
