{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fae442b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Importing important libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Functions import (Basic_info_func, Remove_outliers_with_lof, Select_k_best_features, Adjusted_r2_score,\n",
    "                       Evaluation_results)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.activations import tanh\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import statsmodels.api as sm\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.feature_selection import mutual_info_regression, SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f6ad9",
   "metadata": {},
   "source": [
    "Path = /OneDrive/Desktop/MS-AAi/Course_500_Probability/Project_AAI500-A1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b89890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset \n",
    "train_df = pd.read_csv('./Data/train_df.csv')\n",
    "test_df = pd.read_csv('./Data/test_df.csv')\n",
    "\n",
    "X_train = train_df.drop('critical_temp', axis = 1)\n",
    "y_train = train_df['critical_temp']\n",
    "\n",
    "X_test = test_df.drop('critical_temp', axis = 1)\n",
    "y_test = test_df['critical_temp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28bb97ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before outlier removal:\n",
      "(18073, 82)\n",
      "\n",
      "Shape after outlier removal:\n",
      "(16265, 82)\n"
     ]
    }
   ],
   "source": [
    "new_train_X, new_train_y  = Remove_outliers_with_lof(X_train, y_train, contamination = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e29a48b",
   "metadata": {},
   "source": [
    "#### Features Selection\n",
    "\n",
    "In the data analysis part we observed that our entire data has many highly colinear features that causes multi colinearity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca0a88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train_X and test_X are your training and test datasets\n",
    "pca = PCA(n_components=30) \n",
    "# 1. Fit scaler (or PCA) on training data\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(new_train_X)\n",
    "train_X_pca = pca.fit_transform(train_X_scaled)\n",
    "\n",
    "# 2. Transform both training and test data using the fitted scaler (or PCA)\n",
    "train_X_pca = pca.transform(train_X_scaled)\n",
    "test_X_scaled = scaler.transform(X_test)\n",
    "test_X_pca = pca.transform(test_X_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc65cf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Results\n",
      "\n",
      " - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Training results:\n",
      "Training RMSE: 18.89155\n",
      "Training MAE: 14.69187\n",
      "Training R2 score: 0.69736\n",
      "Training Adjusted R2 score: 0.69680\n",
      "\n",
      " - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Testing results:\n",
      "Testing RMSE: 19.90200\n",
      "Testing MAE: 15.57628\n",
      "Testing R2 score: 0.66114\n",
      "Testing Adjusted R2 score: 0.65792\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize the linear regression model\n",
    "simple_linear_regression = LinearRegression()\n",
    "\n",
    "# Step 2: Fit the model on the scaled training data\n",
    "simple_linear_regression.fit(train_X_pca, new_train_y)\n",
    "\n",
    "# Step 3: Predict on the training set\n",
    "train_preds = simple_linear_regression.predict(train_X_pca)\n",
    "\n",
    "# Testing results\n",
    "print('Linear Regression Results')\n",
    "num_features = train_X_pca.shape[1]\n",
    "train_metrics = Evaluation_results(new_train_y, train_preds, objective = 'train', num_features  = num_features)\n",
    "train_metrics\n",
    "\n",
    "# # Testing Results\n",
    "test_preds = simple_linear_regression.predict(test_X_pca)\n",
    "test_metrics = Evaluation_results(y_test, test_preds, objective = 'test', num_features  = num_features)\n",
    "test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02a0915a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 849.1191 - mae: 21.1562 - mse: 849.1191 - val_loss: 436.0328 - val_mae: 14.6757 - val_mse: 436.0328\n",
      "Epoch 2/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 489.3618 - mae: 16.1159 - mse: 489.3618 - val_loss: 529.7430 - val_mae: 17.6841 - val_mse: 529.7430\n",
      "Epoch 3/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 518.5899 - mae: 16.8951 - mse: 518.5899 - val_loss: 509.0418 - val_mae: 16.2764 - val_mse: 509.0418\n",
      "Epoch 4/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 514.1879 - mae: 16.8569 - mse: 514.1879 - val_loss: 501.2987 - val_mae: 15.5664 - val_mse: 501.2987\n",
      "Epoch 5/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 506.4456 - mae: 16.6379 - mse: 506.4456 - val_loss: 502.4236 - val_mae: 16.5817 - val_mse: 502.4236\n",
      "Epoch 6/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 507.9764 - mae: 16.6072 - mse: 507.9764 - val_loss: 454.8479 - val_mae: 15.6218 - val_mse: 454.8479\n",
      "Epoch 7/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 515.2128 - mae: 16.6836 - mse: 515.2128 - val_loss: 505.0429 - val_mae: 16.5034 - val_mse: 505.0429\n",
      "Epoch 8/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 482.4248 - mae: 16.1984 - mse: 482.4248 - val_loss: 489.4560 - val_mae: 16.6641 - val_mse: 489.4560\n",
      "Epoch 9/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 498.5154 - mae: 16.6183 - mse: 498.5154 - val_loss: 531.2214 - val_mae: 16.2151 - val_mse: 531.2214\n",
      "Epoch 10/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 505.3709 - mae: 16.8226 - mse: 505.3709 - val_loss: 511.2885 - val_mae: 16.8274 - val_mse: 511.2885\n",
      "Epoch 11/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 495.4919 - mae: 16.4315 - mse: 495.4919 - val_loss: 504.0456 - val_mae: 16.3484 - val_mse: 504.0456\n",
      "Epoch 12/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 502.0293 - mae: 16.4190 - mse: 502.0293 - val_loss: 438.5958 - val_mae: 15.1678 - val_mse: 438.5958\n",
      "Epoch 13/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 495.6337 - mae: 16.4005 - mse: 495.6337 - val_loss: 477.3122 - val_mae: 16.1505 - val_mse: 477.3122\n",
      "Epoch 14/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 508.4993 - mae: 16.7144 - mse: 508.4993 - val_loss: 481.9376 - val_mae: 16.1000 - val_mse: 481.9376\n",
      "Epoch 15/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 481.3683 - mae: 16.0991 - mse: 481.3683 - val_loss: 503.0765 - val_mae: 16.3236 - val_mse: 503.0765\n",
      "Epoch 16/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 497.6062 - mae: 16.3343 - mse: 497.6062 - val_loss: 473.3105 - val_mae: 15.8717 - val_mse: 473.3105\n",
      "Epoch 17/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 494.1723 - mae: 16.2661 - mse: 494.1723 - val_loss: 482.0938 - val_mae: 16.1038 - val_mse: 482.0938\n",
      "Epoch 18/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 492.3356 - mae: 16.3478 - mse: 492.3356 - val_loss: 487.2791 - val_mae: 16.0763 - val_mse: 487.2791\n",
      "Epoch 19/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 490.3873 - mae: 16.1853 - mse: 490.3873 - val_loss: 478.3233 - val_mae: 15.9871 - val_mse: 478.3233\n",
      "Epoch 20/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 487.0870 - mae: 16.3108 - mse: 487.0870 - val_loss: 533.9166 - val_mae: 16.4335 - val_mse: 533.9166\n",
      "Epoch 21/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 507.7799 - mae: 16.6021 - mse: 507.7799 - val_loss: 492.4363 - val_mae: 15.9420 - val_mse: 492.4363\n",
      "Epoch 22/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 501.7506 - mae: 16.3495 - mse: 501.7506 - val_loss: 488.4402 - val_mae: 16.4087 - val_mse: 488.4402\n",
      "Epoch 23/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 496.0232 - mae: 16.3064 - mse: 496.0232 - val_loss: 488.5426 - val_mae: 15.8683 - val_mse: 488.5426\n",
      "Epoch 24/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 473.8144 - mae: 15.8365 - mse: 473.8144 - val_loss: 482.3116 - val_mae: 16.2509 - val_mse: 482.3116\n",
      "Epoch 25/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 472.4944 - mae: 15.9351 - mse: 472.4944 - val_loss: 477.2610 - val_mae: 15.8936 - val_mse: 477.2610\n",
      "Epoch 26/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 499.8886 - mae: 16.3446 - mse: 499.8886 - val_loss: 495.6421 - val_mae: 16.6318 - val_mse: 495.6421\n",
      "Epoch 27/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 496.5538 - mae: 16.2777 - mse: 496.5538 - val_loss: 510.3094 - val_mae: 16.9871 - val_mse: 510.3094\n",
      "Epoch 28/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 492.4636 - mae: 16.1825 - mse: 492.4636 - val_loss: 500.0399 - val_mae: 16.2454 - val_mse: 500.0399\n",
      "Epoch 29/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 508.3782 - mae: 16.5145 - mse: 508.3782 - val_loss: 482.0630 - val_mae: 15.9264 - val_mse: 482.0630\n",
      "Epoch 30/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 506.4629 - mae: 16.3702 - mse: 506.4629 - val_loss: 492.7805 - val_mae: 16.0768 - val_mse: 492.7805\n",
      "Epoch 31/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 504.2675 - mae: 16.4371 - mse: 504.2675 - val_loss: 504.0863 - val_mae: 16.5252 - val_mse: 504.0863\n",
      "Epoch 32/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 501.0179 - mae: 16.4794 - mse: 501.0179 - val_loss: 505.9643 - val_mae: 16.2169 - val_mse: 505.9643\n",
      "Epoch 33/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 492.6253 - mae: 16.1818 - mse: 492.6253 - val_loss: 495.1634 - val_mae: 16.2337 - val_mse: 495.1634\n",
      "Epoch 34/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 504.7468 - mae: 16.4029 - mse: 504.7468 - val_loss: 500.4077 - val_mae: 16.1403 - val_mse: 500.4077\n",
      "Epoch 35/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 504.8576 - mae: 16.2656 - mse: 504.8576 - val_loss: 493.8653 - val_mae: 16.9028 - val_mse: 493.8653\n",
      "Epoch 36/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 493.3825 - mae: 16.2150 - mse: 493.3825 - val_loss: 489.3175 - val_mae: 15.9915 - val_mse: 489.3175\n",
      "Epoch 37/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 497.6744 - mae: 16.2034 - mse: 497.6744 - val_loss: 481.1529 - val_mae: 16.0559 - val_mse: 481.1529\n",
      "Epoch 38/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 497.2055 - mae: 16.3526 - mse: 497.2055 - val_loss: 512.3058 - val_mae: 16.5118 - val_mse: 512.3058\n",
      "Epoch 39/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 497.6124 - mae: 16.2676 - mse: 497.6124 - val_loss: 487.4283 - val_mae: 16.1386 - val_mse: 487.4283\n",
      "Epoch 40/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 495.6068 - mae: 16.0966 - mse: 495.6068 - val_loss: 493.7975 - val_mae: 15.9322 - val_mse: 493.7975\n",
      "Epoch 41/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 492.8001 - mae: 16.0967 - mse: 492.8001 - val_loss: 492.2615 - val_mae: 15.8733 - val_mse: 492.2615\n",
      "Epoch 42/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 487.6782 - mae: 15.9288 - mse: 487.6782 - val_loss: 461.7568 - val_mae: 15.5231 - val_mse: 461.7568\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 492.4550 - mae: 16.1404 - mse: 492.4550 - val_loss: 501.2061 - val_mae: 15.9886 - val_mse: 501.2061\n",
      "Epoch 44/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 493.9358 - mae: 16.1363 - mse: 493.9358 - val_loss: 497.0743 - val_mae: 15.9989 - val_mse: 497.0743\n",
      "Epoch 45/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 496.6648 - mae: 16.0591 - mse: 496.6648 - val_loss: 490.9183 - val_mae: 16.1636 - val_mse: 490.9183\n",
      "Epoch 46/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 478.2502 - mae: 15.9725 - mse: 478.2502 - val_loss: 489.7637 - val_mae: 16.0426 - val_mse: 489.7637\n",
      "Epoch 47/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 487.8615 - mae: 16.1093 - mse: 487.8615 - val_loss: 489.6772 - val_mae: 15.9470 - val_mse: 489.6772\n",
      "Epoch 48/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 488.4494 - mae: 16.1385 - mse: 488.4494 - val_loss: 499.3875 - val_mae: 16.4507 - val_mse: 499.3875\n",
      "Epoch 49/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 484.8365 - mae: 16.2491 - mse: 484.8365 - val_loss: 480.2735 - val_mae: 15.8097 - val_mse: 480.2735\n",
      "Epoch 50/50\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 474.8416 - mae: 15.8743 - mse: 474.8416 - val_loss: 486.5097 - val_mae: 16.1788 - val_mse: 486.5097\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 957us/step\n",
      "Neuaral Network Results\n",
      "\n",
      " - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Training results:\n",
      "Training RMSE: 22.34257\n",
      "Training MAE: 16.09114\n",
      "Training R2 score: 0.57669\n",
      "Training Adjusted R2 score: 0.57591\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\n",
      " - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Testing results:\n",
      "Testing RMSE: 22.88044\n",
      "Testing MAE: 17.07342\n",
      "Testing R2 score: 0.55213\n",
      "Testing Adjusted R2 score: 0.54787\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network architecture\n",
    "xgb_model = Sequential([\n",
    "    Dense(512, input_shape=(train_X_pca.shape[1],), activation=tanh),\n",
    "    Dropout(0.4),\n",
    "    Dense(256, activation=tanh),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation=tanh),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation=tanh),\n",
    "    Dense(32, activation=tanh),\n",
    "    Dense(1)  # Output layer for regression task\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model with appropriate optimizer and loss function\n",
    "xgb_model.compile(optimizer=Adam(learning_rate=0.05),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae', 'mse'])\n",
    "\n",
    "# Train the model\n",
    "history = xgb_model.fit(train_X_pca, new_train_y, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate on training set\n",
    "NN_train_preds = model.predict(train_X_pca)\n",
    "\n",
    "print('Neuaral Network Results')\n",
    "num_features = train_X_pca.shape[1]\n",
    "train_metrics = Evaluation_results(new_train_y, NN_train_preds, objective = 'train', num_features  = num_features)\n",
    "train_metrics\n",
    "\n",
    "# Step 5: Predict on the testing set\n",
    "NN_test_preds = xgb_model.predict(test_X_pca)\n",
    "\n",
    "num_features = train_X_pca.shape[1]\n",
    "train_metrics = Evaluation_results(y_test, NN_test_preds, objective = 'test', num_features  = num_features)\n",
    "train_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a086980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results\n",
      "\n",
      " - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Training results:\n",
      "Training RMSE: 4.70945\n",
      "Training MAE: 2.48831\n",
      "Training R2 score: 0.98119\n",
      "Training Adjusted R2 score: 0.98116\n",
      "\n",
      " - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Testing results:\n",
      "Testing RMSE: 11.27645\n",
      "Testing MAE: 6.26300\n",
      "Testing R2 score: 0.89121\n",
      "Testing Adjusted R2 score: 0.89018\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize the XGBoost Regressor model\n",
    "# using RandomSearchCV Best parameters: {'subsample': 0.6, 'n_estimators': 1000, 'max_depth': 10, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=1000, learning_rate=0.01, max_depth=10,reg_lambda = 0.6,\n",
    "                             random_state=42)\n",
    "\n",
    "# Step 2: Fit the model on the scaled training data\n",
    "xgb_model.fit(train_X_pca, new_train_y)\n",
    "\n",
    "# Step 3: Predict on the training set\n",
    "xgb_train_preds = xgb_model.predict(train_X_pca)\n",
    "\n",
    "# Training results\n",
    "print('XGBoost Results')\n",
    "num_features = train_X_pca.shape[1]\n",
    "train_metrics = Evaluation_results(new_train_y, xgb_train_preds, objective = 'train', num_features  = num_features)\n",
    "train_metrics\n",
    "\n",
    "# Predict on the testing set\n",
    "xgb_test_preds = xgb_model.predict(test_X_pca)\n",
    "\n",
    "# Testing results\n",
    "num_features = train_X_pca.shape[1]\n",
    "train_metrics = Evaluation_results(y_test, xgb_test_preds, objective = 'test', num_features  = num_features)\n",
    "train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e121d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbbe44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
